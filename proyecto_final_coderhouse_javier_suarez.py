# -*- coding: utf-8 -*-
"""Proyecto Final CoderHouse_Javier_Suarez.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pIcrrVp2YsQ3kuuO-Mzlzq9FJYfTITYh

¿Podemos predecir la popularidad de una nueva pista en Spotify antes de su lanzamiento?

¿Qué características musicales influyen más en la popularidad de una pista en Spotify?

Esta pregunta busca identificar qué características específicas de las pistas musicales (como danceability, energy, loudness, valence, etc.) tienen una mayor influencia en su popularidad. Puedes utilizar técnicas de análisis de correlación y visualizaciones para descubrir estas relaciones.
¿Podemos predecir la popularidad de una nueva pista en Spotify antes de su lanzamiento?

Esta pregunta implica la construcción de un modelo de machine learning capaz de predecir la popularidad de una pista musical en base a sus características antes de que sea lanzada. Puedes utilizar algoritmos de regresión o clasificación para crear el modelo y evaluar su rendimiento.
"""



"""# Data Adquisition

"""

import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import plotly.express as px
import scipy.stats as stats
import warnings

# Ignorar todas las advertencias
warnings.filterwarnings("ignore")

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd '/content/drive/MyDrive/Notebooks entregas Coder'

"""DataSet de Tracks de Spotify descargados de:
https://www.kaggle.com/datasets/lehaknarnauli/spotify-datasets?select=tracks.csv

Veamos un ejemplo de nuestro DataFrame.
"""

df = pd.read_csv('/content/drive/MyDrive/Notebooks entregas Coder/DataSetSpotify.csv')

df.sample(3)

"""# Descripcion de columnas y variables

# Preprocesamiento de los Datos

Veamos la estructura de DataFrame
"""

df.shape

df.info()

"""Buscamos los valores Nulos"""

df.isnull().sum().sum()

"""tenemos 71 valores ausentes en la variable 'name', como es un numero insignificante frente al conjunto de datos vamos a eliminarlos"""

df.dropna(subset=['name'], axis=0, inplace=True)

"""Valores Duplicados"""

df.duplicated().sum().sum()

"""Veamos las columnas o variables"""

df.columns

"""Empecemos por eliminar las columnas id y id_artists que no nos aportan al analisis"""

df = df.drop(['id','id_artists'], axis = 1)

"""Convertir los milisegundos en segundos"""

df['duration'] = df['duration_ms'].apply(lambda x:round(x/1000))
df.drop('duration_ms', inplace=True, axis=1)
df.duration.head(10)

"""Cambiamos el tipo de dato a formato fecha"""

df['release_date'] = pd.to_datetime(df['release_date'])
df['release_date'].dtype

"""Veamos como queda nuestro Data Frame"""

df.head(5)

"""Descripción de las columnas/Variables

name : Nombre de la pista.

artists(artista) : Los nombres de los artistas que interpretaron la pista. Si hay más de un artista, se separan por un;

popularity : La popularidad de una pista es un valor entre 0 y 100, siendo 100 la más popular . La popularidad se calcula mediante un algoritmo y se basa, en su mayor parte, en el número total de reproducciones que ha tenido la pista y cuán recientes son esas reproducciones. En términos generales, las canciones que se tocan mucho ahora tendrán una mayor popularidad que las canciones que se tocaban mucho en el pasado. Las pistas duplicadas (por ejemplo, la misma pista de un sencillo y un álbum) se clasifican de forma independiente. La popularidad del artista y del álbum se deriva matemáticamente de la popularidad de la pista.

duration_ms : La duración de la pista en milisegundos

explícit : Si la pista tiene o no letras explícitas (verdadero = sí, falso = no, no O desconocido)

dancebility : La bailabilidad describe qué tan adecuada es una pista para bailar en función de una combinación de elementos musicales que incluyen tempo, estabilidad del ritmo, fuerza del ritmo y regularidad general. Un valor de 0,0 es menos bailable y 1,0 es más bailable

energy : La energía es una medida de 0,0 a 1,0 y representa una medida perceptiva de intensidad y actividad. Por lo general, las pistas enérgicas se sienten rápidas, fuertes y ruidosas. Por ejemplo, el death metal tiene mucha energía, mientras que un preludio de Bach puntúa bajo en la escala.

loudness: la sonoridad general de una pista en decibelios (dB). Los valores de sonoridad se promedian en toda la pista y son útiles para comparar la sonoridad relativa de las pistas. El volumen es la cualidad de un sonido que es el principal correlato psicológico de la fuerza física (amplitud). Los valores típicos oscilan entre -60 y 0 db.

key : La clave en la que se encuentra la pista. Los números enteros se asignan a tonos utilizando la notación de clase de tono estándar. Por ejemplo 0 = C, 1 = C♯/D♭, 2 = D, y así sucesivamente. Si no se detectó ninguna clave, el valor es -1

mode : Mode indica la modalidad (mayor o menor) de una pista, el tipo de escala de la que se deriva su contenido melódico. Mayor está representado por 1 y menor es 0

Speechiness : Speechiness detecta la presencia de palabras habladas en una pista. Cuanto más se parece exclusivamente al habla la grabación (por ejemplo, programa de entrevistas, audiolibro, poesía), más cerca de 1,0 es el valor del atributo. Los valores superiores a 0,66 describen pistas que probablemente estén formadas en su totalidad por palabras habladas. Los valores entre 0,33 y 0,66 describen pistas que pueden contener tanto música como voz, ya sea en secciones o en capas, incluidos casos como la música rap. Los valores por debajo de 0,33 probablemente representen música y otras pistas que no sean de voz.

acústica : Una medida de confianza de 0,0 a 1,0 de si la pista es acústica. 1.0 representa una alta confianza en que la pista es acústica

instrumentalness : Predice si una pista no contiene voces. Los sonidos "Ooh" y "aah" se tratan como instrumentales en este contexto. Las pistas de rap o de palabras habladas son claramente "vocales". Cuanto más cerca esté el valor de instrumentalidad de 1,0, mayor será la probabilidad de que la pista no contenga contenido vocal.

liveness : Detecta la presencia de una audiencia en la grabación. Los valores de vivacidad más altos representan una mayor probabilidad de que la pista se interprete en vivo. Un valor por encima de 0,8 proporciona una gran probabilidad de que la pista esté en vivo

valences : Una medida de 0.0 a 1.0 que describe la positividad musical transmitida por una pista. Las pistas con una valencia alta suenan más positivas (p. ej., felices, alegres, eufóricas), mientras que las pistas con una valencia baja suenan más negativas (p. ej., tristes, deprimidas, enojadas).

tempo : El tempo general estimado de una pista en pulsaciones por minuto (BPM). En terminología musical, el tempo es la velocidad o ritmo de una pieza dada y se deriva directamente de la duración promedio del tiempo.

time_signature : una firma de tiempo estimada. El compás (medidor) es una convención de notación para especificar cuántos tiempos hay en cada compás (o compás). La signatura de compás varía de 3 a 7, lo que indica signaturas de compás de 3/4, a 7/4.

Analisis Descriptivo
"""

df.describe().T

"""Veamos graficamente con histogramas las distribuciones de nuestras variables"""

df.hist(bins = 20, color = 'green', figsize = (20, 14))

"""Junto al analisis descriptivo y a las graficas de las distribuciones de las variables, podemos sacar algunas concluciones generales del dataset:


"""



"""# Data Wrangling

Veamos la distribucion de las canciones en el tiempo
"""

# Crear una nueva columna llamada "year" que contenga solo el año de cada fecha
df['year'] = df['release_date'].dt.year

# Contar la frecuencia de cada año
year_counts = df['year'].value_counts()

# Ordenar los años de forma ascendente
year_counts = year_counts.sort_index()

# Graficar la frecuencia de cada año
plt.plot(year_counts.index, year_counts.values)
plt.xlabel('Año')
plt.ylabel('Tracks')

# Pintar el área debajo de la curva en verde
plt.fill_between(year_counts.index, year_counts.values, color='green', alpha=0.3)

plt.show()

df['year'] = pd.DatetimeIndex(df['release_date']).year
sns.boxplot(data=df, x='year')

# Agrupar por año y contar las ocurrencias
counts = df.groupby('year')['year'].count()

# Convertir a array de numpy y guardar en una variable
year_counts = np.array(list(zip(counts.index.tolist(), counts.tolist())))

# Mostrar resultado
print(year_counts)

"""Sobre la fecha de realizacion tenemos algunas certezas pero tambien muchas dudas. Sabemos que el dataset con el correr de los años a tenido un aumento de tracks que la mayoria de los tracks se encuentran entre los 70 y 2010, que ha tenido caidas pronunciadas como en 1966. Tambien sabemos que los datos llegan hasta abril del 2021 y que sus picos vienen siendo los ultimos años. Pero tambien nnos preguntamos, sobre las canciones que fueron compuestas antes, ejemplo la musica clasica, esta editada la fecha segun el interprete? Cuanto tiene que ver la carga a la plataforma con el tiempo de realizacion?. Estas y otras preguntas no son parte del analisis pero las dejaremos planteadas.

Veamos las variables categoricas
"""

df['explicit'].value_counts()

# contar valores
explicit_counts = df['explicit'].value_counts()

# definir etiquetas
labels = ['No explicito', 'Explicito']

# definir colores
colors = ['green', 'red']

# graficar
plt.pie(explicit_counts, labels=labels, colors=colors, autopct='%1.1f%%')
plt.axis('equal')
plt.title('Conteo de canciones explicitas')
plt.show()

df['mode'].value_counts()

df['mode'].value_counts().plot(kind='bar', color=['blue', 'orange'], rot=0)
plt.xlabel('Modo')
plt.ylabel('Número de canciones')
plt.xticks([0, 1], ['Mayor', 'Menor'])
plt.show()

df['time_signature'].value_counts()

sns.countplot(data=df, x='time_signature')

# diccionario que asigne cada valor a su nota correspondiente

notas = {0: 'Do', 1: 'Do#', 2: 'Re', 3: 'Re#', 4: 'Mi', 5: 'Fa', 6: 'Fa#', 7: 'Sol', 8: 'Sol#', 9: 'La', 10: 'La#', 11: 'Si'}

df['key'] = df['key'].replace(notas)

df['key'].value_counts().sort_index()

# Diccionario que asigne cada valor a su nota correspondiente
notas = {0: 'Do', 1: 'Do#', 2: 'Re', 3: 'Re#', 4: 'Mi', 5: 'Fa', 6: 'Fa#', 7: 'Sol', 8: 'Sol#', 9: 'La', 10: 'La#', 11: 'Si'}

# Graficar la cuenta de valores con un color distinto para cada nota
plt.bar(df['key'].value_counts().index, df['key'].value_counts().values, color=plt.cm.tab20(range(len(notas))))

# Etiquetar los ejes
plt.xlabel('Nota')
plt.ylabel('Frecuencia')

# Mostrar el gráfico
plt.show()

df['name'].describe()

df['artists'].describe()

"""veamos las medidas de tendencia central y dispersion de las variables numericas"""

import scipy
stats.describe(df['popularity'])

"""En este caso, podemos ver que hay 586,601 observaciones en el conjunto de datos, con un valor mínimo de 0 y un valor máximo de 100. La media de los valores es de aproximadamente 27.57, y la varianza es de aproximadamente 337.44. La asimetría de los valores es ligeramente positiva, lo que sugiere que hay más valores a la izquierda de la media. La curtosis es negativa, lo que indica que la distribución es ligeramente más aplanada que una distribución normal."""

stats.describe(df['loudness'])

"""Los valores oscilan entre -60.0 y 5.376. El promedio de la columna es de -10.205788933193089, y la varianza es de 25.90221975000921. La asimetría de la distribución es negativa, lo que sugiere que la cola de la distribución se extiende hacia la izquierda, y el kurtosis es de 2.717866846371617, lo que indica una distribución más picuda que la distribución normal."""

stats.describe(df['energy'])

"""En este caso, la variable parece tener una media cercana a 0.54, lo que podría indicar una tendencia a tomar valores más altos. La varianza es relativamente baja, lo que sugiere que los valores tienden a estar bastante agrupados alrededor de la media. El coeficiente de asimetría es ligeramente negativo, lo que indica que la distribución de los valores es ligeramente sesgada hacia la izquierda. El coeficiente de curtosis es negativo, lo que sugiere que la distribución de los valores es más achatada que una distribución normal."""

stats.describe(df['danceability'])

"""Los valores oscilan entre 0.0 y 0.991. La media es 0.5636, lo que indica que la mayoría de los valores están cerca de este valor. La varianza es relativamente baja en 0.0275, lo que sugiere que los valores están bastante concentrados en torno a la media.

La asimetría es negativa en -0.3310, lo que indica que la distribución es ligeramente sesgada hacia la izquierda. El coeficiente de curtosis es negativo en -0.2738, lo que sugiere que la distribución es ligeramente menos puntiaguda que la distribución normal. En resumen, la distribución parece estar relativamente concentrada alrededor de la media y no presenta una asimetría o curtosis extremas.
"""

stats.describe(df['valence'])

"""los datos tienen una media de 0.55, una varianza de 0.066, y una distribución sesgada hacia la izquierda (skewness negativo). El kurtosis negativo indica que la distribución es relativamente plana en comparación con una distribución normal."""

stats.describe(df['acousticness'])

"""Para estos datos, el valor mínimo es 0 y el valor máximo es 0.996. El promedio de todos los valores es de 0.4498, lo que sugiere que los valores tienden a estar más cercanos a 0 que a 1. La varianza es de 0.1216, lo que indica que los valores están bastante dispersos. El coeficiente de asimetría es de 0.1513, lo que indica que la distribución es ligeramente sesgada hacia la derecha. El coeficiente de curtosis es de -1.466, lo que indica que la distribución es relativamente plana en comparación con una distribución normal."""

stats.describe(df['instrumentalness'])

"""En estos datos, se observa que el valor mínimo es 0.0 y el valor máximo es 1.0. La media es de 0.113, lo que indica que en promedio, los valores son bajos. La varianza es de 0.071, lo que sugiere que hay una dispersión moderada en los datos. La asimetría (skewness) es de 2.270, lo que sugiere que los datos están sesgados hacia valores más bajos. El coeficiente de kurtosis es de 3.549, lo que indica que la distribución de los datos es más puntiaguda (distribución leptocúrtica) que la distribución normal, lo que sugiere una mayor concentración de datos en los extremos"""

stats.describe(df['liveness'])

"""Los datos muestran una distribución bastante sesgada hacia valores más altos (skewness=2.04), lo que sugiere que la mayoría de los valores están por encima de la media. El alto valor de kurtosis (4.29) sugiere que la distribución tiene colas muy pesadas, lo que significa que hay un número relativamente grande de valores extremadamente altos en comparación con una distribución normal. El rango de valores (minmax) es de 0 a 1, lo que sugiere que se trata de una proporción o porcentaje, y la varianza es relativamente baja, lo que indica que los valores están bastante agrupados en torno a la media."""

stats.describe(df['speechiness'])

"""Los valores mínimo y máximo en la muestra son 0.0 y 0.971, respectivamente. La media de la muestra es 0.10487, lo que indica que en promedio los valores en la muestra son relativamente bajos. La varianza de la muestra es 0.03236, lo que sugiere que la muestra tiene una distribución relativamente estrecha alrededor de la media. La asimetría de la distribución de la muestra es 3.6937, lo que indica que hay una cola pesada a la derecha en la distribución. La curtosis de la distribución es 13.4152, lo que sugiere que hay una gran cantidad de valores atípicos en la distribución."""

stats.describe(df['instrumentalness'])

"""Los valores mínimos y máximos en la variable son 0.0 y 1.0, respectivamente. La media de la variable es 0.1134. La varianza es 0.0712. La asimetría de la distribución es positiva (2.27), lo que indica que la cola de la distribución se extiende hacia la derecha. La curtosis es de 3.55, lo que indica que la distribución es más puntiaguda que la distribución normal."""

stats.describe(df['duration'])

"""la variable "duration", con una duración mínima de 3 segundos y máxima de 5621 segundos. El promedio de duración de las canciones es de 230.05 segundos, con una varianza de 16010.55 segundos cuadrados. La distribución de los datos está sesgada hacia la derecha, con un valor de asimetría de 10.33 y un alto valor de curtosis de 241.04, lo que indica que hay una gran cantidad de valores atípicos y que la distribución no es normal."""

stats.describe(df['tempo'])

"""En promedio, las canciones tienen un tempo de 118.47 beats por minuto (BPM). La varianza es alta, lo que sugiere que hay una gran variabilidad en el tempo de las canciones en el conjunto de datos. La asimetría es ligeramente positiva, lo que sugiere que la distribución puede estar ligeramente sesgada hacia la derecha, aunque esto no es muy pronunciado. La curtosis es negativa, lo que sugiere que la distribución puede ser relativamente plana y amplia en comparación con una distribución normal.

# EDA

**Veamos ahora algunas relaciones entre las variables de nuestro data set**

Correlación entre "popularity" y otras variables, como "danceability", "energy", "valence", "loudness", etc., para ver si hay una relación positiva o negativa entre estas variables y la popularidad de las canciones.

Análisis de la relación entre la "acousticness" y "instrumentalness" para ver si las canciones acústicas suelen ser menos instrumentales y viceversa.

Relación entre "danceability" y "tempo" para ver si las canciones más bailables suelen tener un tempo más rápido.

Análisis de la relación entre "loudness" y "energy" para ver si las canciones más ruidosas tienden a tener más energía.
"""

df.describe().T

sns.heatmap(df.corr())

# Calcula la matriz de correlación entre todas las variables
corr_matrix = df.corr()

# Muestra la correlación de cada variable con "popularity"
popularity_corr = corr_matrix['popularity'].sort_values(ascending=False)
print(popularity_corr)

import seaborn as sns

# Gráfico bivariado de dispersión
df_sample = df.sample(1000)
sns.scatterplot(data=df_sample, x="danceability", y="popularity", hue="key")

# Cuenta la frecuencia de cada combinación de variables
counts = df.groupby(['key', 'mode']).size().unstack()

# Suma las filas para obtener la frecuencia total de cada variable key
total_counts = counts.sum(axis=1)

# Ordena los valores en orden descendente
sorted_keys = total_counts.sort_values(ascending=False).index

# Ordena las filas del DataFrame counts según el orden de sorted_keys
counts = counts.loc[sorted_keys]

# Crea el diagrama de barras apilado
counts.plot(kind='bar', stacked=True)

# Muestra el gráfico
plt.show()

# Cuenta la frecuencia de cada combinación de variables
counts = df.groupby(['key', 'time_signature']).size().unstack()

# Suma las filas para obtener la frecuencia total de cada variable key
total_counts = counts.sum(axis=1)

# Ordena los valores en orden descendente
sorted_keys = total_counts.sort_values(ascending=False).index

# Ordena las filas del DataFrame counts según el orden de sorted_keys
counts = counts.loc[sorted_keys]

# Crea el diagrama de barras apilado
counts.plot(kind='bar', stacked=True)

# Muestra el gráfico
plt.show()

# Cuenta la frecuencia de cada combinación de variables
counts = df.groupby(['explicit', 'mode']).size().unstack()

# Suma las filas para obtener la frecuencia total de cada variable key
total_counts = counts.sum(axis=1)

# Ordena los valores en orden descendente
sorted_keys = total_counts.sort_values(ascending=False).index

# Ordena las filas del DataFrame counts según el orden de sorted_keys
counts = counts.loc[sorted_keys]

# Crea el diagrama de barras apilado
counts.plot(kind='bar', stacked=True)

# Muestra el gráfico
plt.show()

# Cuenta la frecuencia de cada combinación de variables
counts = df.groupby(['time_signature', 'mode']).size().unstack()

# Suma las filas para obtener la frecuencia total de cada variable key
total_counts = counts.sum(axis=1)

# Ordena los valores en orden descendente
sorted_keys = total_counts.sort_values(ascending=False).index

# Ordena las filas del DataFrame counts según el orden de sorted_keys
counts = counts.loc[sorted_keys]

# Crea el diagrama de barras apilado
counts.plot(kind='bar', stacked=True)

# Muestra el gráfico
plt.show()

# Cuenta la frecuencia de cada combinación de variables
counts = df.groupby(['key', 'explicit']).size().unstack()

# Suma las filas para obtener la frecuencia total de cada variable key
total_counts = counts.sum(axis=1)

# Ordena los valores en orden descendente
sorted_keys = total_counts.sort_values(ascending=False).index

# Ordena las filas del DataFrame counts según el orden de sorted_keys
counts = counts.loc[sorted_keys]

# Crea el diagrama de barras apilado
counts.plot(kind='bar', stacked=True)

# Muestra el gráfico
plt.show()

sns.boxplot(x=df['key'], y= df['popularity'])
plt.show()

sns.boxplot(x=df['mode'], y= df['popularity'])
plt.show()

sns.boxplot(x=df['time_signature'], y= df['popularity'])
plt.show()

sns.boxplot(x=df['explicit'], y= df['popularity'])
plt.show()

sns.boxplot(x=df['key'], y= df['danceability'])
plt.show()

sns.boxplot(x=df['time_signature'], y= df['danceability'])
plt.show()

sns.boxplot(x=df['explicit'], y= df['danceability'])
plt.show()

sns.boxplot(x=df['mode'], y= df['danceability'])
plt.show()

sns.boxplot(x=df['key'], y= df['energy'])
plt.show()

sns.boxplot(x=df['explicit'], y= df['energy'])
plt.show()

sns.boxplot(x=df['time_signature'], y= df['energy'])
plt.show()

sns.boxplot(x=df['time_signature'], y= df['valence'])
plt.show()

sns.boxplot(x=df['key'], y= df['valence'])
plt.show()

sns.boxplot(x=df['mode'], y= df['valence'])
plt.show()

sns.boxplot(x=df['explicit'], y= df['valence'])
plt.show()

sns.boxplot(x=df['time_signature'], y= df['valence'])
plt.show()

df_sample.plot(x='popularity', y='danceability')

# Establece las etiquetas de los ejes y el título del gráfico
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Relación entre X e Y')

# Muestra el gráfico resultante
plt.show()

import seaborn as sns

# Selecciona las columnas numéricas
num_cols = ['popularity', 'explicit', 'danceability', 'energy', 'key',
            'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness',
            'liveness', 'valence', 'tempo', 'time_signature', 'duration']

# Crea un gráfico pairplot con tres columnas
sns.pairplot(df[num_cols], x_vars=num_cols, y_vars=['popularity'], height=3, plot_kws=dict(s=10, edgecolor="b", linewidth=0.5), diag_kws=dict(bins=20))

# Muestra el gráfico resultante
plt.show()

"""**Trabajaremos con un dataset filtrado de las 1000 pistas mas populares**"""

# Seleccionar la columna que deseas ordenar
columna = df['popularity']

# Ordenar el dataframe por la columna seleccionada de forma descendente
df_ordenado = df.sort_values(by='popularity', ascending=False)

# Seleccionar los primeros 1000 registros de la columna ordenada
df_filtrado = df_ordenado.head(1000)

df_filtrado.head()

df_filtrado.shape

df_filtrado.corr().loc[:, 'popularity']

sns.heatmap(df_filtrado.corr())

df_filtrado.describe().T

# contar valores
explicit_counts = df_filtrado['explicit'].value_counts()

# definir etiquetas
labels = ['No explicito', 'Explicito']

# definir colores
colors = ['green', 'red']

# graficar
plt.pie(explicit_counts, labels=labels, colors=colors, autopct='%1.1f%%')
plt.axis('equal')
plt.title('Conteo de canciones explicitas')
plt.show()

# Diccionario que asigne cada valor a su nota correspondiente
notas = {0: 'Do', 1: 'Do#', 2: 'Re', 3: 'Re#', 4: 'Mi', 5: 'Fa', 6: 'Fa#', 7: 'Sol', 8: 'Sol#', 9: 'La', 10: 'La#', 11: 'Si'}

# Graficar la cuenta de valores con un color distinto para cada nota
plt.bar(df_filtrado['key'].value_counts().index, df_filtrado['key'].value_counts().values, color=plt.cm.tab20(range(len(notas))))

# Etiquetar los ejes
plt.xlabel('Nota')
plt.ylabel('Frecuencia')

# Mostrar el gráfico
plt.show()

sns.countplot(data=df_filtrado, x='time_signature')

df_filtrado['mode'].value_counts().plot(kind='bar', color=['blue', 'orange'], rot=0)
plt.xlabel('Modo')
plt.ylabel('Número de canciones')
plt.xticks([0, 1], ['Mayor', 'Menor'])
plt.show()

df['instrumentalness'].value_counts()

"""Conclusiones:

Insights
*El lenguaje explícito hace que la popularidad, la danzabilidad y la energía aumenten.
*En la estructura musical, las primeras cuatro notas mantienen una dispersión acumulada, pero en las más populares las notas cambian en tres de cuatro. Siendo, DO# la más utilizada.
*La Valencia, o estado de ánimo no parece mantener una corelación directa con la popularidad, ya que baja de 0,55 a 0,44 en las más populares.
*El Time Signature se mantiene en general y los modos también, prevaleciendo los mayores.
*Parecería haber una correlación inversa entre la acústica y la instrumentalidad con la popularidad.
*La media de duración de las canciones populares es de 200’ y se mantiene cercana a la media general que es de 230.
Recomendaciones:
*Si lo que se busca es posicionar mejor una pista en Spotify, se sugiere trabajar en composiciones en 4 tiempos, utilizando las notas DO#, SI, DO y FA#, que contengan lenguaje explícito y una alta energía y bailabilidad.
*Se sugiere reducir el uso de instrumentales y acústicos, mantener una duración de no más de 200’ y no abusar del uso de lenguaje hablado.
*Tener en cuenta que Spotify mide la popularidad entre las reproducciones totales y las actuales, por tanto, mantener actualizado el catálogo de un artista tiene una correlación directa con su popularidad en la app

#Regresion Lineal y Regresion Lineal Multiple
"""

#importamos las librerias necesarias
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from scipy.stats import ttest_ind
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# Seleccionar las características (features) y la variable objetivo (target)
features = ['explicit', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']
target = 'popularity'

# Dividir los datos en características (X) y variable objetivo (y)
X = df_sample[features]
y = df_sample[target]

# ColumnTransformer para codificar la variable categórica "key"
column_transformer = ColumnTransformer(
    [('encoder', OneHotEncoder(), ['key'])],  # columnas a codificar
    remainder='passthrough'  # mantener las demás columnas sin modificar
)
X_encoded = column_transformer.fit_transform(X)

# Escalar las características utilizando StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)


# Dividir los datos escalados en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Entrenar el modelo de regresión lineal
regression_model = LinearRegression()
regression_model.fit(X_train, y_train)

# Realizar predicciones en el conjunto de prueba
y_pred = regression_model.predict(X_test)

# Evaluar el modelo utilizando varias métricas de regresión
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print('Error cuadrático medio (MSE):', mse)
print('Error absoluto medio (MAE):', mae)
print('Coeficiente de determinación (R^2):', r2)

# Visualizar los resultados
plt.scatter(y_test, y_pred)
plt.xlabel('Valor verdadero')
plt.ylabel('Predicción')
plt.title('Comparación entre los valores verdaderos y las predicciones')
plt.show()

"""*Ahora lo hacemos sobre el data set filtrado o las 1000 pistas populares"""

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# Seleccionar las características (features) y la variable objetivo (target)
features = ['explicit', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']
target = 'popularity'

# Dividir los datos en características (X) y variable objetivo (y)
X = df_filtrado[features]
y = df_filtrado[target]

# ColumnTransformer para codificar la variable categórica "key"
column_transformer = ColumnTransformer(
    [('encoder', OneHotEncoder(), ['key'])],  # columnas a codificar
    remainder='passthrough'  # mantener las demás columnas sin modificar
)
X_encoded = column_transformer.fit_transform(X)

# Escalar las características utilizando StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)


# Dividir los datos escalados en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Entrenar el modelo de regresión lineal
regression_model = LinearRegression()
regression_model.fit(X_train, y_train)

# Realizar predicciones en el conjunto de prueba
y_pred = regression_model.predict(X_test)

# Evaluar el modelo utilizando varias métricas de regresión
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print('Error cuadrático medio (MSE):', mse)
print('Error absoluto medio (MAE):', mae)
print('Coeficiente de determinación (R^2):', r2)

# Visualizar los resultados
plt.scatter(y_test, y_pred)
plt.xlabel('Valor verdadero')
plt.ylabel('Predicción')
plt.title('Comparación entre los valores verdaderos y las predicciones')
plt.show()

"""En términos de los errores cuadráticos medios y los errores absolutos medios, el modelo ajustado en df_filtrado muestra un mejor rendimiento en comparación con el modelo en df. Sin embargo, es importante tener en cuenta que los valores de R^2 en ambos casos son bajos, lo que sugiere que ambos modelos tienen dificultades para explicar la variabilidad en la popularidad. Puede ser necesario realizar ajustes adicionales en los modelos o explorar enfoques alternativos para mejorar su capacidad de predicción de la popularidad"""

#continua...



"""# Encoding y Escalamiento"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder
from scipy.stats import boxcox, yeojohnson

"""Variables categoricas: 'key', 'time_signature','year', 'name','artists', realease_date'
Variables Binarias: 'explicit', 'mode'

variables numericas continuas
"""

df_numericas = df.select_dtypes(include=['int64', 'float64'])
df_numericas.columns

unique_values_counts = df_numericas.apply(lambda x: len(x.unique()))
unique_values_counts.sort_values(ascending=True)

df_numericas = df_numericas.drop(['explicit', 'mode', 'time_signature'], axis=1)
df_numericas.columns

"""##Ver cuales cumplen los supuestos de una curva normal"""

import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats


# Variables de interés
variables = df_numericas.columns

# Verificación gráfica de normalidad
for variable in variables:
    # Gráfico de histograma
    plt.hist(df_numericas[variable], bins='auto', alpha=0.7)
    plt.xlabel(variable)
    plt.ylabel('Frecuencia')
    plt.title('Histograma de ' + variable)
    plt.show()

    # Gráfico Q-Q
    stats.probplot(df_numericas[variable], dist="norm", plot=plt)
    plt.title('Gráfico Q-Q de ' + variable)
    plt.xlabel('Cuantiles teóricos')
    plt.ylabel('Cuantiles de ' + variable)
    plt.show()

from scipy.stats import shapiro

# Prueba de normalidad de Shapiro-Wilk
for variable in variables:
    _, p_value = shapiro(df_numericas[variable])
    print(f'La variable {variable} tiene un valor de p de {p_value}')
    if p_value > 0.05:
        print(f'La variable {variable} sigue una distribución normal')
    else:
        print(f'La variable {variable} no sigue una distribución normal')

from scipy.stats import skew, kurtosis

for variable in variables:
    skewness = skew(df_numericas[variable])
    kurt = kurtosis(df_numericas[variable])

    print(f'Variable: {variable}')
    print(f'  Asimetría (skewness): {skewness}')
    print(f'  Curtosis (kurtosis): {kurt}')

"""Las variables popularity, danceability, energy, loudness, acousticness, valence, y tempo tienen valores de p cercanos a cero y asimetría y curtosis cercanas a cero, lo que indica que no siguen una distribución normal.

Las variables speechiness, instrumentalness, liveness, y duration también tienen valores de p cercanos a cero, pero sus valores de asimetría y curtosis son bastante altos, indicando que tienen colas pesadas y no siguen una distribución normal.
"""

import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import numpy as np

# Lista de columnas numéricas que deseas transformar con la transformación logarítmica
columns_to_transform_log = ['speechiness', 'instrumentalness', 'liveness', 'duration']

# Aplicar la transformación logarítmica a las columnas
for column in columns_to_transform_log:
    df_filtrado[column] = np.log1p(df_filtrado[column])

# Visualización de histogramas
for column in columns_to_transform_log:
    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.title(f'Histograma original - {column}')
    sns.histplot(df_filtrado[column], kde=True)

    plt.subplot(1, 2, 2)
    plt.title(f'Histograma transformado - {column}')
    sns.histplot(df_filtrado[column], kde=True)
    plt.show()

# Visualización de gráficos de densidad
for column in columns_to_transform_log:
    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.title(f'Gráfico de densidad original - {column}')
    sns.kdeplot(df_filtrado[column])

    plt.subplot(1, 2, 2)
    plt.title(f'Gráfico de densidad transformado - {column}')
    sns.kdeplot(df_filtrado[column])
    plt.show()

# Visualización de gráficos Q-Q
for column in columns_to_transform_log:
    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.title(f'Gráfico Q-Q original - {column}')
    stats.probplot(df_filtrado[column], dist='norm', plot=plt)

    plt.subplot(1, 2, 2)
    plt.title(f'Gráfico Q-Q transformado - {column}')
    stats.probplot(df_filtrado[column], dist='norm', plot=plt)
    plt.show()

# Prueba de normalidad de Shapiro-Wilk
for column in columns_to_transform_log:
    stat, p = stats.shapiro(df_filtrado[column])
    print(f"Variable: {column}")
    print(f"  Valor p: {p}")
    if p < 0.05:
        print(f"  La variable {column} no sigue una distribución normal después de la transformación logarítmica.")
    else:
        print(f"  La variable {column} sigue una distribución normal después de la transformación logarítmica.")

# Prueba de normalidad de D'Agostino-Pearson
for column in columns_to_transform_log:
    stat, p = stats.normaltest(df_filtrado[column])
    print(f"Variable: {column}")
    print(f"  Valor p: {p}")
    if p < 0.05:
        print(f"  La variable {column} no sigue una distribución normal después de la transformación logarítmica.")
    else:
        print(f"  La variable {column} sigue una distribución normal después de la transformación logarítmica.")

import numpy as np

# Lista de columnas numéricas que deseas transformar con la transformación logarítmica
columns_to_transform_log = ['speechiness', 'instrumentalness', 'liveness', 'duration']

# Aplicar la transformación logarítmica a las columnas
for column in columns_to_transform_log:
    df_filtrado[column] = np.log1p(df_filtrado[column])

from scipy.stats import ttest_ind
from scipy.stats import f_oneway

# Realizar pruebas t de Student entre "popularity" y cada variable predictoras después de la transformación logarítmica
for column in columns_to_transform_log:
    t_stat, p_value = ttest_ind(df_filtrado[column], df_filtrado['popularity'])
    print(f"Prueba t de Student entre 'popularity' y '{column}':")
    print(f"  Estadística t: {t_stat}")
    print(f"  Valor p: {p_value}")
    if p_value < 0.05:
        print(f"  La variable {column} tiene una relación significativa con 'popularity' después de la transformación logarítmica.")
    else:
        print(f"  La variable {column} no tiene una relación significativa con 'popularity' después de la transformación logarítmica.")

# Realizar pruebas ANOVA entre "popularity" y cada variable predictoras después de la transformación logarítmica
for column in columns_to_transform_log:
    f_stat, p_value = f_oneway(df_filtrado[column], df_filtrado['popularity'])
    print(f"Prueba ANOVA entre 'popularity' y '{column}':")
    print(f"  Estadística F: {f_stat}")
    print(f"  Valor p: {p_value}")
    if p_value < 0.05:
        print(f"  La variable {column} tiene una relación significativa con 'popularity' después de la transformación logarítmica.")
    else:
        print(f"  La variable {column} no tiene una relación significativa con 'popularity' después de la transformación logarítmica.")

"""Interpretación general:

Después de aplicar la transformación logarítmica a las variables "speechiness", "instrumentalness", "liveness" y "duration", todas ellas muestran una relación significativa con la popularidad de las pistas en Spotify.
Esto sugiere que estas características transformadas tienen una influencia importante en la popularidad de una pista musical

Tamaño de muestra:
Si tienes un tamaño de muestra grande, los supuestos de normalidad pueden ser menos críticos. Puedes evaluar la robustez de tus análisis utilizando métodos no paramétricos o de remuestreo. A continuación, se muestra un ejemplo de cómo realizar una prueba no paramétrica, como la prueba de Mann-Whitney, entre dos variables
"""

from scipy.stats import spearmanr, kendalltau

variables = ['popularity', 'danceability', 'energy', 'loudness', 'speechiness',
             'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',
             'duration']

# Calcular correlaciones de Spearman y Kendall para todas las combinaciones de variables
for i in range(len(variables)):
    for j in range(i + 1, len(variables)):
        variable1 = df_numericas[variables[i]]
        variable2 = df_numericas[variables[j]]

        # Correlación de Spearman
        spearman_corr, spearman_pvalue = spearmanr(variable1, variable2)
        print(f'Correlación de Spearman entre {variables[i]} y {variables[j]}: Coeficiente = {spearman_corr}, p-value = {spearman_pvalue}')

        # Correlación de Kendall
        kendall_corr, kendall_pvalue = kendalltau(variable1, variable2)
        print(f'Correlación de Kendall entre {variables[i]} y {variables[j]}: Coeficiente = {kendall_corr}, p-value = {kendall_pvalue}')

import seaborn as sns
import matplotlib.pyplot as plt

# Calcular la matriz de correlación
correlation_matrix = df_numericas.corr()

# Generar el mapa de calor
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Mapa de calor de correlaciones')
plt.show()

"""Correlaciones con "popularity":

Las variables "danceability", "energy", "loudness", "acousticness", "instrumentalness" y "valence" tienen correlaciones positivas significativas con la variable "popularity". Esto sugiere que a medida que estas variables aumentan, la popularidad de las pistas en Spotify tiende a aumentar.
Por otro lado, las variables "speechiness", "liveness", "tempo" y "duration" tienen correlaciones negativas significativas con la variable "popularity". Esto indica que a medida que estas variables aumentan, la popularidad de las pistas tiende a disminuir.
Correlaciones entre características musicales:

Existen algunas correlaciones significativas entre características musicales. Por ejemplo, "energy" tiene una correlación positiva con "loudness" y "valence". También hay una correlación negativa entre "acousticness" e "instrumentalness".
"""

# Normalizamos con max-min scaler nuestras variables nuemericas continuas
from sklearn.preprocessing import RobustScaler

# Crear un nuevo DataFrame con solo las columnas seleccionadas
df_escalado_nuemericas = df_numericas

# Aplicar RobustScaler a las columnas seleccionadas
robust_scaler = RobustScaler()
robust_scaled_data = robust_scaler.fit_transform(df_escalado_nuemericas)

# Imprimir los datos escalados
print("Robust Scaled Data:\n", robust_scaled_data)

df_escalado_nuemericas.head()

df_escalado_nuemericas.columns

"""Vamos aplicar el encoding a las variables categoricas y a las ordinales

Usamos el Ordinal Encoding ya que las notas son ordinales.(estas variables las codeamos para hacer el EDA, ahora volvemos a realizar el encoding
"""

df['key'].value_counts()

from scipy.stats import mannwhitneyu

variable1 = df_numericas['energy']
variable2 = df_numericas['valence']

statistic, p_value = mannwhitneyu(variable1, variable2)
print(f'Prueba de Mann-Whitney: Estadístico = {statistic}, p-value = {p_value}')

"""Ahora vayamos a las variables categoricas name artists ,year, etc"""

df.columns

"""Ahora para las variables categoricas, hemos aplicado el encoding por etiquetas"""

le = LabelEncoder()
df["name_encoded"] = le.fit_transform(df["name"])

le = LabelEncoder()
df["artists_encoded"] = le.fit_transform(df["artists"])

le = LabelEncoder()
df["release_date_encoded"] = le.fit_transform(df["release_date"])

df.shape

df_escalado_nuemericas.columns

df_categoricas = df[['name_encoded','artists_encoded', 'release_date_encoded']]

df.info()

df_escalado = pd.concat([df_escalado_nuemericas, df_categoricas], axis=1)

df_escalado.info()

#Veamos la correlacion en un heatmap con el df_escalado ya encodeado o estandarizado
import seaborn as sns

correlation_matrix = df_escalado.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Correlación entre variables en df_escalado")
plt.show()

df_escalado.corr().loc['popularity', ].sort_values()

"""# División de datos en conjuntos de entrenamiento y prueba




"""

from sklearn.model_selection import train_test_split

# Dividir el conjunto de datos en características (X) y variable objetivo (y)
X = df_escalado.drop('popularity', axis=1)
y = df_escalado['popularity']

# Dividir los datos en conjunto de entrenamiento y conjunto de prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Modelos de Clasificacion

"""

df_escalado[['popularity']].describe()

"""Usamos los rangos interquartiles para generar tres categorias: Alta, Media, Baja."""

# Obtener los cuartiles de la columna 'popularity'
quartiles = df_escalado['popularity'].quantile([0, 0.25, 0.5, 0.75, 1])

# Obtener los valores de los cuartiles
q1, q2, q3 = quartiles[0.25], quartiles[0.5], quartiles[0.75]

# Generar las etiquetas de las categorías
categorias = ['baja', 'media', 'alta']

# Crear una nueva columna 'Ranking' e inicializarla con la categoría 'media' para todos los registros
df_escalado['Ranking'] = 'media'

# Asignar las categorías 'baja', 'media' o 'alta' según los cuartiles
df_escalado.loc[df_escalado['popularity'] <= q1, 'Ranking'] = 'baja'
df_escalado.loc[(df_escalado['popularity'] > q1) & (df_escalado['popularity'] <= q3), 'Ranking'] = 'media'
df_escalado.loc[df_escalado['popularity'] > q3, 'Ranking'] = 'alta'


print(df_escalado[['popularity', 'Ranking']])

unique_values = df_escalado['Ranking'].unique()
print(unique_values)

"""encodiemos nuestra variable nueva"""

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

df_escalado['Ranking_encoded'] = label_encoder.fit_transform(df_escalado['Ranking'])

# Mostrar el DataFrame con la columna 'popularity_category_encoded'
print(df_escalado[['Ranking', 'Ranking_encoded']])

#Borramos la columna ranking
df_escalado.drop('Ranking', axis=1, inplace=True)

df_escalado.sample(1) #DAta set escalado y codificado listo para trabajar en ML

print(df_escalado.columns)
print(df.shape)

data = df_escalado[['popularity', 'danceability', 'energy', 'loudness', 'speechiness',
       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',
       'duration', 'year', 'name_encoded', 'artists_encoded',
       'release_date_encoded', 'name_encoded', 'artists_encoded',
       'release_date_encoded', 'Ranking_encoded']]

data.sample(1)

data.describe()

""" Evaluacion del conjunto de Prueba"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Dividir el conjunto de datos en características (X) y variable objetivo (y)
X = data.drop('Ranking_encoded', axis=1)
y = data['Ranking_encoded']

# Dividir los datos en conjunto de entrenamiento y conjunto de prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#  modelo de Regresión Logística
logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)

#  modelo de Vecinos más Cercanos
knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)

#  modelo de Árbol de Decisión
dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)

#  modelo de Análisis Discriminante Lineal
lda_model = LinearDiscriminantAnalysis()
lda_model.fit(X_train, y_train)

#  modelo de Naive Bayes
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

#  modelo de Random Forest
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)

# Obtener las predicciones para cada modelo
logistic_predictions = logistic_model.predict(X_test)
knn_predictions = knn_model.predict(X_test)
dt_predictions = dt_model.predict(X_test)
lda_predictions = lda_model.predict(X_test)
nb_predictions = nb_model.predict(X_test)
rf_predictions = rf_model.predict(X_test)

# Obtener el reporte de clasificación para cada modelo
print("Reporte de clasificación (Regresión Logística):")
print(classification_report(y_test, logistic_predictions))

print("Reporte de clasificación (Vecinos más Cercanos):")
print(classification_report(y_test, knn_predictions))

print("Reporte de clasificación (Árbol de Decisión):")
print(classification_report(y_test, dt_predictions))

print("Reporte de clasificación (Análisis Discriminante Lineal):")
print(classification_report(y_test, lda_predictions))

print("Reporte de clasificación (Naive Bayes):")
print(classification_report(y_test, nb_predictions))

print("Reporte de clasificación (Bosques Aleatorios):")
print(classification_report(y_test, rf_predictions))

"""analicemos las métricas para cada modelo:

Regresión Logística:

Precisión alta en la categoría "1", pero menor en las categorías "0" y "2".
Recall alto en la categoría "2", pero menor en las categorías "0" y "1".
F1-score alto en la categoría "2", pero menor en las categorías "0" y "1".
En general, tiene una precisión y recall bajos en comparación con otros modelos.
Vecinos más Cercanos:

Precisión y recall similares en las tres categorías, pero un poco más bajos en comparación con otros modelos.
F1-score balanceado en las tres categorías, pero también ligeramente más bajos.
Árbol de Decisión:

Tiene una precisión, recall y f1-score perfectos en todas las categorías, lo cual parece poco realista. Puede ser un indicio de sobreajuste en el conjunto de entrenamiento.
Análisis Discriminante Lineal:

Buen rendimiento en todas las métricas y categorías, con precisión y recall muy altos.
Naive Bayes:

Precisión alta en todas las categorías, especialmente en "1".
Recall más bajo en la categoría "1", pero balanceado en las demás categorías.
F1-score alto en todas las categorías, pero ligeramente más bajo en "1".
Bosques Aleatorios:

Tiene una precisión, recall y f1-score perfectos en todas las categorías, lo cual también puede indicar sobreajuste.
Dado que estamos buscando un modelo que se ajuste bien a nuestro problema de negocio, uno de los enfoques más adecuados es seleccionar el modelo que tenga un buen balance entre precisión y recall, especialmente en la categoría "1", ya que esta categoría representa el ranking de popularidad medio.

En base a estos resultados, el modelo de Análisis Discriminante Lineal parece tener un buen rendimiento en todas las métricas y categorías, y es una opción sólida para predecir el ranking de popularidad en nuestras categorías alta, media y baja.
"""

# Supongamos que X_test es el conjunto de prueba y y_test es el vector de etiquetas reales

index_example = 0
example = X_test.iloc[index_example, :]  # Obtener el ejemplo como una fila del DataFrame

# Utilizar el modelo LDA entrenado para predecir la clase del ejemplo
prediction = lda_model.predict([example])

# Comparar la predicción con la etiqueta real del ejemplo
real_label = y_test.iloc[index_example]
print("Ejemplo:")
print("Características:", example)
print("Etiqueta Real:", real_label)
print("Predicción del Modelo:", prediction[0])

"""# Validacion y Ajuste de hiperparametros

Realizaremos la prueba de chi-cuadrado para evaluar la asociación entre cada variable categórica y la variable objetivo (ranking de popularidad).
"""

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import classification_report
from scipy.stats import chi2_contingency

# Variables categóricas
categorical_features = ['name_encoded', 'artists_encoded', 'release_date_encoded']

# Eliminar columnas duplicadas
data_unique = data.loc[:, ~data.columns.duplicated()]

# Calculamos la prueba de chi-cuadrado para cada variable categórica
selected_categorical_features = []
for feature in categorical_features:
    contingency_table = pd.crosstab(data_unique[feature], data_unique['Ranking_encoded'])
    chi2, p_value, _, _ = chi2_contingency(contingency_table)
    if p_value < 0.05:  # Si el p-valor es menor a 0.05, consideramos la variable como relevante
        selected_categorical_features.append(feature)

# Variables numéricas
numeric_features = ['popularity', 'danceability', 'energy', 'loudness', 'speechiness',
                    'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',
                    'duration', 'year']

# Calculamos la correlación de Pearson para cada variable numérica
selected_numeric_features = []
for feature in numeric_features:
    correlation = data_unique[feature].corr(data_unique['Ranking_encoded'])
    if abs(correlation) > 0.1:  # Si la correlación es mayor a 0.1 (positiva o negativa), consideramos la variable como relevante
        selected_numeric_features.append(feature)

# Combinar las características seleccionadas
selected_features = selected_categorical_features + selected_numeric_features

# Dividir los datos en conjunto de entrenamiento y conjunto de prueba
X = data_unique[selected_features]
y = data_unique['Ranking_encoded']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Escalado de las variables numéricas seleccionadas
scaler = StandardScaler()
X_train[selected_numeric_features] = scaler.fit_transform(X_train[selected_numeric_features])
X_test[selected_numeric_features] = scaler.transform(X_test[selected_numeric_features])

# Ajustar los hiperparámetros del modelo LDA utilizando una búsqueda de cuadrícula
param_grid = {
    'solver': ['svd', 'lsqr', 'eigen'],
    # Agrega otros hiperparámetros específicos del modelo LDA si es necesario
}

lda_model = LinearDiscriminantAnalysis()
grid_search = GridSearchCV(lda_model, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Obtener los mejores hiperparámetros
best_params = grid_search.best_params_

# Entrenar el modelo LDA con los mejores hiperparámetros
lda_model_best = LinearDiscriminantAnalysis(**best_params)
lda_model_best.fit(X_train, y_train)

# Obtener las predicciones para el conjunto de prueba
y_pred = lda_model_best.predict(X_test)

# Obtener el reporte de clasificación para evaluar el rendimiento del modelo
print("Reporte de clasificación (Análisis Discriminante Lineal):")
print(classification_report(y_test, y_pred))

"""Los resultados del modelo de Análisis Discriminante Lineal (LDA) son muy buenos. En general, el modelo tiene un alto rendimiento en todas las métricas de evaluación, como precisión, recall y f1-score, para todas las categorías. Esto sugiere que el modelo es capaz de hacer predicciones precisas y confiables para clasificar las canciones en las categorías de popularidad alta, media y baja.

Hemos seleccionado las características relevantes utilizando las pruebas estadísticas para variables categóricas y numéricas, y también hemos escalado las variables numéricas seleccionadas. Luego, hemos ajustado los hiperparámetros del modelo LDA utilizando la búsqueda de cuadrícula y hemos obtenido el reporte de clasificación para evaluar el rendimiento del modelo

Ahora que hemos realizado la selección de características, podemos ajustar los hiperparámetros del modelo LDA utilizando una búsqueda de cuadrícula (grid search) con validación cruzada:
"""

from sklearn.model_selection import GridSearchCV

# Definir los hiperparámetros a ajustar
param_grid = {
    'solver': ['svd', 'lsqr', 'eigen'],
    # Agrega otros hiperparámetros específicos del modelo LDA si es necesario
}

# Inicializar el modelo LDA
lda_model = LinearDiscriminantAnalysis()

# Realizar la búsqueda de cuadrícula con validación cruzada
grid_search = GridSearchCV(lda_model, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Obtener los mejores hiperparámetros
best_params = grid_search.best_params_

# Entrenar el modelo LDA con los mejores hiperparámetros
lda_model_best = LinearDiscriminantAnalysis(**best_params)
lda_model_best.fit(X_train, y_train)

# Obtener las predicciones para el conjunto de prueba
y_pred = lda_model_best.predict(X_test)

# Obtener el reporte de clasificación para evaluar el rendimiento del modelo
print("Reporte de clasificación (Análisis Discriminante Lineal):")
print(classification_report(y_test, y_pred))

from sklearn.model_selection import GridSearchCV

# Definir la cuadrícula de hiperparámetros a probar
param_grid = {
    'solver': ['svd', 'lsqr', 'eigen'],  # Diferentes métodos para ajustar el modelo LDA
    'shrinkage': [None, 'auto', 0.1, 0.5, 0.9],  # Parámetro de reducción (solo para 'lsqr' y 'eigen')
}

# Inicializar el modelo LDA
lda_model = LinearDiscriminantAnalysis()

# Realizar la búsqueda de cuadrícula con validación cruzada
grid_search = GridSearchCV(lda_model, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Obtener los mejores hiperparámetros
best_params = grid_search.best_params_

# Imprimir los mejores hiperparámetros
print("Mejores hiperparámetros:", best_params)

"""los hiperparámetros optimizados para el modelo LDA, podemos usarlos para ajustar el modelo con estos valores y evaluar su rendimiento en el conjunto de prueba."""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import classification_report, accuracy_score

# Mejores hiperparámetros para el modelo LDA
best_solver = 'lsqr'
best_shrinkage = 'auto'

# Inicializar el modelo LDA con los mejores hiperparámetros
lda_model_tuned = LinearDiscriminantAnalysis(solver=best_solver, shrinkage=best_shrinkage)

# Ajustar el modelo en el conjunto de entrenamiento
lda_model_tuned.fit(X_train, y_train)

# Predecir las etiquetas del conjunto de prueba
y_pred_test = lda_model_tuned.predict(X_test)

# Obtener la precisión del modelo en el conjunto de prueba
accuracy = accuracy_score(y_test, y_pred_test)

# Obtener el reporte de clasificación en el conjunto de prueba
classification_report_test = classification_report(y_test, y_pred_test)

# Imprimir la precisión y el reporte de clasificación
print("Precisión del modelo en el conjunto de prueba:", accuracy)
print("Reporte de clasificación en el conjunto de prueba:\n", classification_report_test)

"""Con los hiperparámetros optimizados, el modelo LDA ha mostrado un rendimiento muy bueno en el conjunto de prueba. La precisión del 97.41% indica que el modelo es capaz de predecir correctamente la categoría de popularidad en la mayoría de las instancias.

El reporte de clasificación también es muy positivo, con altos valores de precisión, recall y F1-score en todas las categorías. Esto significa que el modelo es capaz de clasificar correctamente las canciones en las tres categorías de popularidad: baja, media y alta, con un buen equilibrio entre precisión y recall.
"""