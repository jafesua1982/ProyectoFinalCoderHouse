# -*- coding: utf-8 -*-
"""Modelo de Regresion Lineal Multiple  _Javier_Suarez.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pIcrrVp2YsQ3kuuO-Mzlzq9FJYfTITYh

¿Podemos predecir la popularidad de una nueva pista en Spotify antes de su lanzamiento?

¿Qué características musicales influyen más en la popularidad de una pista en Spotify?

Esta pregunta busca identificar qué características específicas de las pistas musicales (como danceability, energy, loudness, valence, etc.) tienen una mayor influencia en su popularidad. Puedes utilizar técnicas de análisis de correlación y visualizaciones para descubrir estas relaciones.
¿Podemos predecir la popularidad de una nueva pista en Spotify antes de su lanzamiento?

Esta pregunta implica la construcción de un modelo de machine learning capaz de predecir la popularidad de una pista musical en base a sus características antes de que sea lanzada. Puedes utilizar algoritmos de regresión o clasificación para crear el modelo y evaluar su rendimiento.
"""



"""# Data Adquisition

"""

import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import plotly.express as px
import scipy.stats as stats
import warnings

# Ignorar todas las advertencias
warnings.filterwarnings("ignore")

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd '/content/drive/MyDrive/Notebooks entregas Coder'

"""DataSet de Tracks de Spotify descargados de:
https://www.kaggle.com/datasets/lehaknarnauli/spotify-datasets?select=tracks.csv

Veamos un ejemplo de nuestro DataFrame.
"""

df = pd.read_csv('/content/drive/MyDrive/Notebooks entregas Coder/DataSetSpotify.csv')

df.sample(3)

"""# Descripcion de columnas y variables

# Preprocesamiento de los Datos

Veamos la estructura de DataFrame
"""

df.shape

df.info()

"""Buscamos los valores Nulos"""

df.isnull().sum().sum()

"""tenemos 71 valores ausentes en la variable 'name', como es un numero insignificante frente al conjunto de datos vamos a eliminarlos"""

df.dropna(subset=['name'], axis=0, inplace=True)

"""Valores Duplicados"""

df.duplicated().sum().sum()

"""Veamos las columnas o variables"""

df.columns

"""Empecemos por eliminar las columnas id y id_artists que no nos aportan al analisis"""

df = df.drop(['id','id_artists'], axis = 1)

"""Convertir los milisegundos en segundos"""

df['duration'] = df['duration_ms'].apply(lambda x:round(x/1000))
df.drop('duration_ms', inplace=True, axis=1)
df.duration.head(10)

"""Cambiamos el tipo de dato a formato fecha"""

df['release_date'] = pd.to_datetime(df['release_date'])
df['release_date'].dtype

"""Veamos como queda nuestro Data Frame"""

df.head(5)

"""Descripción de las columnas/Variables

name : Nombre de la pista.

artists(artista) : Los nombres de los artistas que interpretaron la pista. Si hay más de un artista, se separan por un;

popularity : La popularidad de una pista es un valor entre 0 y 100, siendo 100 la más popular . La popularidad se calcula mediante un algoritmo y se basa, en su mayor parte, en el número total de reproducciones que ha tenido la pista y cuán recientes son esas reproducciones. En términos generales, las canciones que se tocan mucho ahora tendrán una mayor popularidad que las canciones que se tocaban mucho en el pasado. Las pistas duplicadas (por ejemplo, la misma pista de un sencillo y un álbum) se clasifican de forma independiente. La popularidad del artista y del álbum se deriva matemáticamente de la popularidad de la pista.

duration_ms : La duración de la pista en milisegundos

explícit : Si la pista tiene o no letras explícitas (verdadero = sí, falso = no, no O desconocido)

dancebility : La bailabilidad describe qué tan adecuada es una pista para bailar en función de una combinación de elementos musicales que incluyen tempo, estabilidad del ritmo, fuerza del ritmo y regularidad general. Un valor de 0,0 es menos bailable y 1,0 es más bailable

energy : La energía es una medida de 0,0 a 1,0 y representa una medida perceptiva de intensidad y actividad. Por lo general, las pistas enérgicas se sienten rápidas, fuertes y ruidosas. Por ejemplo, el death metal tiene mucha energía, mientras que un preludio de Bach puntúa bajo en la escala.

loudness: la sonoridad general de una pista en decibelios (dB). Los valores de sonoridad se promedian en toda la pista y son útiles para comparar la sonoridad relativa de las pistas. El volumen es la cualidad de un sonido que es el principal correlato psicológico de la fuerza física (amplitud). Los valores típicos oscilan entre -60 y 0 db.

key : La clave en la que se encuentra la pista. Los números enteros se asignan a tonos utilizando la notación de clase de tono estándar. Por ejemplo 0 = C, 1 = C♯/D♭, 2 = D, y así sucesivamente. Si no se detectó ninguna clave, el valor es -1

mode : Mode indica la modalidad (mayor o menor) de una pista, el tipo de escala de la que se deriva su contenido melódico. Mayor está representado por 1 y menor es 0

Speechiness : Speechiness detecta la presencia de palabras habladas en una pista. Cuanto más se parece exclusivamente al habla la grabación (por ejemplo, programa de entrevistas, audiolibro, poesía), más cerca de 1,0 es el valor del atributo. Los valores superiores a 0,66 describen pistas que probablemente estén formadas en su totalidad por palabras habladas. Los valores entre 0,33 y 0,66 describen pistas que pueden contener tanto música como voz, ya sea en secciones o en capas, incluidos casos como la música rap. Los valores por debajo de 0,33 probablemente representen música y otras pistas que no sean de voz.

acústica : Una medida de confianza de 0,0 a 1,0 de si la pista es acústica. 1.0 representa una alta confianza en que la pista es acústica

instrumentalness : Predice si una pista no contiene voces. Los sonidos "Ooh" y "aah" se tratan como instrumentales en este contexto. Las pistas de rap o de palabras habladas son claramente "vocales". Cuanto más cerca esté el valor de instrumentalidad de 1,0, mayor será la probabilidad de que la pista no contenga contenido vocal.

liveness : Detecta la presencia de una audiencia en la grabación. Los valores de vivacidad más altos representan una mayor probabilidad de que la pista se interprete en vivo. Un valor por encima de 0,8 proporciona una gran probabilidad de que la pista esté en vivo

valences : Una medida de 0.0 a 1.0 que describe la positividad musical transmitida por una pista. Las pistas con una valencia alta suenan más positivas (p. ej., felices, alegres, eufóricas), mientras que las pistas con una valencia baja suenan más negativas (p. ej., tristes, deprimidas, enojadas).

tempo : El tempo general estimado de una pista en pulsaciones por minuto (BPM). En terminología musical, el tempo es la velocidad o ritmo de una pieza dada y se deriva directamente de la duración promedio del tiempo.

time_signature : una firma de tiempo estimada. El compás (medidor) es una convención de notación para especificar cuántos tiempos hay en cada compás (o compás). La signatura de compás varía de 3 a 7, lo que indica signaturas de compás de 3/4, a 7/4.

Analisis Descriptivo
"""

df.describe().T

"""Veamos graficamente con histogramas las distribuciones de nuestras variables"""

df.hist(bins = 20, color = 'green', figsize = (20, 14))

"""Junto al analisis descriptivo y a las graficas de las distribuciones de las variables, podemos sacar algunas concluciones generales del dataset:


"""



"""# Data Wrangling

Veamos la distribucion de las canciones en el tiempo
"""

# Crear una nueva columna llamada "year" que contenga solo el año de cada fecha
df['year'] = df['release_date'].dt.year

# Contar la frecuencia de cada año
year_counts = df['year'].value_counts()

# Ordenar los años de forma ascendente
year_counts = year_counts.sort_index()

# Graficar la frecuencia de cada año
plt.plot(year_counts.index, year_counts.values)
plt.xlabel('Año')
plt.ylabel('Tracks')

# Pintar el área debajo de la curva en verde
plt.fill_between(year_counts.index, year_counts.values, color='green', alpha=0.3)

plt.show()

df['year'] = pd.DatetimeIndex(df['release_date']).year
sns.boxplot(data=df, x='year')

# Agrupar por año y contar las ocurrencias
counts = df.groupby('year')['year'].count()

# Convertir a array de numpy y guardar en una variable
year_counts = np.array(list(zip(counts.index.tolist(), counts.tolist())))

# Mostrar resultado
print(year_counts)

"""Sobre la fecha de realizacion tenemos algunas certezas pero tambien muchas dudas. Sabemos que el dataset con el correr de los años a tenido un aumento de tracks que la mayoria de los tracks se encuentran entre los 70 y 2010, que ha tenido caidas pronunciadas como en 1966. Tambien sabemos que los datos llegan hasta abril del 2021 y que sus picos vienen siendo los ultimos años. Pero tambien nnos preguntamos, sobre las canciones que fueron compuestas antes, ejemplo la musica clasica, esta editada la fecha segun el interprete? Cuanto tiene que ver la carga a la plataforma con el tiempo de realizacion?. Estas y otras preguntas no son parte del analisis pero las dejaremos planteadas.

Veamos las variables categoricas
"""

df['explicit'].value_counts()

# contar valores
explicit_counts = df['explicit'].value_counts()

# definir etiquetas
labels = ['No explicito', 'Explicito']

# definir colores
colors = ['green', 'red']

# graficar
plt.pie(explicit_counts, labels=labels, colors=colors, autopct='%1.1f%%')
plt.axis('equal')
plt.title('Conteo de canciones explicitas')
plt.show()

df['mode'].value_counts()

df['mode'].value_counts().plot(kind='bar', color=['blue', 'orange'], rot=0)
plt.xlabel('Modo')
plt.ylabel('Número de canciones')
plt.xticks([0, 1], ['Mayor', 'Menor'])
plt.show()

df['time_signature'].value_counts()

sns.countplot(data=df, x='time_signature')

# diccionario que asigne cada valor a su nota correspondiente

notas = {0: 'Do', 1: 'Do#', 2: 'Re', 3: 'Re#', 4: 'Mi', 5: 'Fa', 6: 'Fa#', 7: 'Sol', 8: 'Sol#', 9: 'La', 10: 'La#', 11: 'Si'}

df['key'] = df['key'].replace(notas)

df['key'].value_counts().sort_index()

# Diccionario que asigne cada valor a su nota correspondiente
notas = {0: 'Do', 1: 'Do#', 2: 'Re', 3: 'Re#', 4: 'Mi', 5: 'Fa', 6: 'Fa#', 7: 'Sol', 8: 'Sol#', 9: 'La', 10: 'La#', 11: 'Si'}

# Graficar la cuenta de valores con un color distinto para cada nota
plt.bar(df['key'].value_counts().index, df['key'].value_counts().values, color=plt.cm.tab20(range(len(notas))))

# Etiquetar los ejes
plt.xlabel('Nota')
plt.ylabel('Frecuencia')

# Mostrar el gráfico
plt.show()

df['name'].describe()

df['artists'].describe()

"""veamos las medidas de tendencia central y dispersion de las variables numericas"""

import scipy
stats.describe(df['popularity'])

"""En este caso, podemos ver que hay 586,601 observaciones en el conjunto de datos, con un valor mínimo de 0 y un valor máximo de 100. La media de los valores es de aproximadamente 27.57, y la varianza es de aproximadamente 337.44. La asimetría de los valores es ligeramente positiva, lo que sugiere que hay más valores a la izquierda de la media. La curtosis es negativa, lo que indica que la distribución es ligeramente más aplanada que una distribución normal."""

stats.describe(df['loudness'])

"""Los valores oscilan entre -60.0 y 5.376. El promedio de la columna es de -10.205788933193089, y la varianza es de 25.90221975000921. La asimetría de la distribución es negativa, lo que sugiere que la cola de la distribución se extiende hacia la izquierda, y el kurtosis es de 2.717866846371617, lo que indica una distribución más picuda que la distribución normal."""

stats.describe(df['energy'])

"""En este caso, la variable parece tener una media cercana a 0.54, lo que podría indicar una tendencia a tomar valores más altos. La varianza es relativamente baja, lo que sugiere que los valores tienden a estar bastante agrupados alrededor de la media. El coeficiente de asimetría es ligeramente negativo, lo que indica que la distribución de los valores es ligeramente sesgada hacia la izquierda. El coeficiente de curtosis es negativo, lo que sugiere que la distribución de los valores es más achatada que una distribución normal."""

stats.describe(df['danceability'])

"""Los valores oscilan entre 0.0 y 0.991. La media es 0.5636, lo que indica que la mayoría de los valores están cerca de este valor. La varianza es relativamente baja en 0.0275, lo que sugiere que los valores están bastante concentrados en torno a la media.

La asimetría es negativa en -0.3310, lo que indica que la distribución es ligeramente sesgada hacia la izquierda. El coeficiente de curtosis es negativo en -0.2738, lo que sugiere que la distribución es ligeramente menos puntiaguda que la distribución normal. En resumen, la distribución parece estar relativamente concentrada alrededor de la media y no presenta una asimetría o curtosis extremas.
"""

stats.describe(df['valence'])

"""los datos tienen una media de 0.55, una varianza de 0.066, y una distribución sesgada hacia la izquierda (skewness negativo). El kurtosis negativo indica que la distribución es relativamente plana en comparación con una distribución normal."""

stats.describe(df['acousticness'])

"""Para estos datos, el valor mínimo es 0 y el valor máximo es 0.996. El promedio de todos los valores es de 0.4498, lo que sugiere que los valores tienden a estar más cercanos a 0 que a 1. La varianza es de 0.1216, lo que indica que los valores están bastante dispersos. El coeficiente de asimetría es de 0.1513, lo que indica que la distribución es ligeramente sesgada hacia la derecha. El coeficiente de curtosis es de -1.466, lo que indica que la distribución es relativamente plana en comparación con una distribución normal."""

stats.describe(df['instrumentalness'])

"""En estos datos, se observa que el valor mínimo es 0.0 y el valor máximo es 1.0. La media es de 0.113, lo que indica que en promedio, los valores son bajos. La varianza es de 0.071, lo que sugiere que hay una dispersión moderada en los datos. La asimetría (skewness) es de 2.270, lo que sugiere que los datos están sesgados hacia valores más bajos. El coeficiente de kurtosis es de 3.549, lo que indica que la distribución de los datos es más puntiaguda (distribución leptocúrtica) que la distribución normal, lo que sugiere una mayor concentración de datos en los extremos"""

stats.describe(df['liveness'])

"""Los datos muestran una distribución bastante sesgada hacia valores más altos (skewness=2.04), lo que sugiere que la mayoría de los valores están por encima de la media. El alto valor de kurtosis (4.29) sugiere que la distribución tiene colas muy pesadas, lo que significa que hay un número relativamente grande de valores extremadamente altos en comparación con una distribución normal. El rango de valores (minmax) es de 0 a 1, lo que sugiere que se trata de una proporción o porcentaje, y la varianza es relativamente baja, lo que indica que los valores están bastante agrupados en torno a la media."""

stats.describe(df['speechiness'])

"""Los valores mínimo y máximo en la muestra son 0.0 y 0.971, respectivamente. La media de la muestra es 0.10487, lo que indica que en promedio los valores en la muestra son relativamente bajos. La varianza de la muestra es 0.03236, lo que sugiere que la muestra tiene una distribución relativamente estrecha alrededor de la media. La asimetría de la distribución de la muestra es 3.6937, lo que indica que hay una cola pesada a la derecha en la distribución. La curtosis de la distribución es 13.4152, lo que sugiere que hay una gran cantidad de valores atípicos en la distribución."""

stats.describe(df['instrumentalness'])

"""Los valores mínimos y máximos en la variable son 0.0 y 1.0, respectivamente. La media de la variable es 0.1134. La varianza es 0.0712. La asimetría de la distribución es positiva (2.27), lo que indica que la cola de la distribución se extiende hacia la derecha. La curtosis es de 3.55, lo que indica que la distribución es más puntiaguda que la distribución normal."""

stats.describe(df['duration'])

"""la variable "duration", con una duración mínima de 3 segundos y máxima de 5621 segundos. El promedio de duración de las canciones es de 230.05 segundos, con una varianza de 16010.55 segundos cuadrados. La distribución de los datos está sesgada hacia la derecha, con un valor de asimetría de 10.33 y un alto valor de curtosis de 241.04, lo que indica que hay una gran cantidad de valores atípicos y que la distribución no es normal."""

stats.describe(df['tempo'])

"""En promedio, las canciones tienen un tempo de 118.47 beats por minuto (BPM). La varianza es alta, lo que sugiere que hay una gran variabilidad en el tempo de las canciones en el conjunto de datos. La asimetría es ligeramente positiva, lo que sugiere que la distribución puede estar ligeramente sesgada hacia la derecha, aunque esto no es muy pronunciado. La curtosis es negativa, lo que sugiere que la distribución puede ser relativamente plana y amplia en comparación con una distribución normal.

# EDA

**Veamos ahora algunas relaciones entre las variables de nuestro data set**

Correlación entre "popularity" y otras variables, como "danceability", "energy", "valence", "loudness", etc., para ver si hay una relación positiva o negativa entre estas variables y la popularidad de las canciones.

Análisis de la relación entre la "acousticness" y "instrumentalness" para ver si las canciones acústicas suelen ser menos instrumentales y viceversa.

Relación entre "danceability" y "tempo" para ver si las canciones más bailables suelen tener un tempo más rápido.

Análisis de la relación entre "loudness" y "energy" para ver si las canciones más ruidosas tienden a tener más energía.
"""

df.describe().T

sns.heatmap(df.corr())

# Calcula la matriz de correlación entre todas las variables
corr_matrix = df.corr()

# Muestra la correlación de cada variable con "popularity"
popularity_corr = corr_matrix['popularity'].sort_values(ascending=False)
print(popularity_corr)

import seaborn as sns

# Gráfico bivariado de dispersión
df_sample = df.sample(1000)
sns.scatterplot(data=df_sample, x="danceability", y="popularity", hue="key")

# Cuenta la frecuencia de cada combinación de variables
counts = df.groupby(['key', 'mode']).size().unstack()

# Suma las filas para obtener la frecuencia total de cada variable key
total_counts = counts.sum(axis=1)

# Ordena los valores en orden descendente
sorted_keys = total_counts.sort_values(ascending=False).index

# Ordena las filas del DataFrame counts según el orden de sorted_keys
counts = counts.loc[sorted_keys]

# Crea el diagrama de barras apilado
counts.plot(kind='bar', stacked=True)

# Muestra el gráfico
plt.show()

# Cuenta la frecuencia de cada combinación de variables
counts = df.groupby(['key', 'time_signature']).size().unstack()

# Suma las filas para obtener la frecuencia total de cada variable key
total_counts = counts.sum(axis=1)

# Ordena los valores en orden descendente
sorted_keys = total_counts.sort_values(ascending=False).index

# Ordena las filas del DataFrame counts según el orden de sorted_keys
counts = counts.loc[sorted_keys]

# Crea el diagrama de barras apilado
counts.plot(kind='bar', stacked=True)

# Muestra el gráfico
plt.show()

# Cuenta la frecuencia de cada combinación de variables
counts = df.groupby(['explicit', 'mode']).size().unstack()

# Suma las filas para obtener la frecuencia total de cada variable key
total_counts = counts.sum(axis=1)

# Ordena los valores en orden descendente
sorted_keys = total_counts.sort_values(ascending=False).index

# Ordena las filas del DataFrame counts según el orden de sorted_keys
counts = counts.loc[sorted_keys]

# Crea el diagrama de barras apilado
counts.plot(kind='bar', stacked=True)

# Muestra el gráfico
plt.show()

# Cuenta la frecuencia de cada combinación de variables
counts = df.groupby(['time_signature', 'mode']).size().unstack()

# Suma las filas para obtener la frecuencia total de cada variable key
total_counts = counts.sum(axis=1)

# Ordena los valores en orden descendente
sorted_keys = total_counts.sort_values(ascending=False).index

# Ordena las filas del DataFrame counts según el orden de sorted_keys
counts = counts.loc[sorted_keys]

# Crea el diagrama de barras apilado
counts.plot(kind='bar', stacked=True)

# Muestra el gráfico
plt.show()

# Cuenta la frecuencia de cada combinación de variables
counts = df.groupby(['key', 'explicit']).size().unstack()

# Suma las filas para obtener la frecuencia total de cada variable key
total_counts = counts.sum(axis=1)

# Ordena los valores en orden descendente
sorted_keys = total_counts.sort_values(ascending=False).index

# Ordena las filas del DataFrame counts según el orden de sorted_keys
counts = counts.loc[sorted_keys]

# Crea el diagrama de barras apilado
counts.plot(kind='bar', stacked=True)

# Muestra el gráfico
plt.show()

sns.boxplot(x=df['key'], y= df['popularity'])
plt.show()

sns.boxplot(x=df['mode'], y= df['popularity'])
plt.show()

sns.boxplot(x=df['time_signature'], y= df['popularity'])
plt.show()

sns.boxplot(x=df['explicit'], y= df['popularity'])
plt.show()

sns.boxplot(x=df['key'], y= df['danceability'])
plt.show()

sns.boxplot(x=df['time_signature'], y= df['danceability'])
plt.show()

sns.boxplot(x=df['explicit'], y= df['danceability'])
plt.show()

sns.boxplot(x=df['mode'], y= df['danceability'])
plt.show()

sns.boxplot(x=df['key'], y= df['energy'])
plt.show()

sns.boxplot(x=df['explicit'], y= df['energy'])
plt.show()

sns.boxplot(x=df['time_signature'], y= df['energy'])
plt.show()

sns.boxplot(x=df['time_signature'], y= df['valence'])
plt.show()

sns.boxplot(x=df['key'], y= df['valence'])
plt.show()

sns.boxplot(x=df['mode'], y= df['valence'])
plt.show()

sns.boxplot(x=df['explicit'], y= df['valence'])
plt.show()

sns.boxplot(x=df['time_signature'], y= df['valence'])
plt.show()

df_sample.plot(x='popularity', y='danceability')

# Establece las etiquetas de los ejes y el título del gráfico
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Relación entre X e Y')

# Muestra el gráfico resultante
plt.show()

import seaborn as sns

# Selecciona las columnas numéricas
num_cols = ['popularity', 'explicit', 'danceability', 'energy', 'key',
            'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness',
            'liveness', 'valence', 'tempo', 'time_signature', 'duration']

# Crea un gráfico pairplot con tres columnas
sns.pairplot(df[num_cols], x_vars=num_cols, y_vars=['popularity'], height=3, plot_kws=dict(s=10, edgecolor="b", linewidth=0.5), diag_kws=dict(bins=20))

# Muestra el gráfico resultante
plt.show()

"""**Trabajaremos con un dataset filtrado de las 1000 pistas mas populares**"""

# Seleccionar la columna que deseas ordenar
columna = df['popularity']

# Ordenar el dataframe por la columna seleccionada de forma descendente
df_ordenado = df.sort_values(by='popularity', ascending=False)

# Seleccionar los primeros 1000 registros de la columna ordenada
df_filtrado = df_ordenado.head(1000)

df_filtrado.head()

df_filtrado.shape

df_filtrado.corr().loc[:, 'popularity']

sns.heatmap(df_filtrado.corr())

df_filtrado.describe().T

# contar valores
explicit_counts = df_filtrado['explicit'].value_counts()

# definir etiquetas
labels = ['No explicito', 'Explicito']

# definir colores
colors = ['green', 'red']

# graficar
plt.pie(explicit_counts, labels=labels, colors=colors, autopct='%1.1f%%')
plt.axis('equal')
plt.title('Conteo de canciones explicitas')
plt.show()

# Diccionario que asigne cada valor a su nota correspondiente
notas = {0: 'Do', 1: 'Do#', 2: 'Re', 3: 'Re#', 4: 'Mi', 5: 'Fa', 6: 'Fa#', 7: 'Sol', 8: 'Sol#', 9: 'La', 10: 'La#', 11: 'Si'}

# Graficar la cuenta de valores con un color distinto para cada nota
plt.bar(df_filtrado['key'].value_counts().index, df_filtrado['key'].value_counts().values, color=plt.cm.tab20(range(len(notas))))

# Etiquetar los ejes
plt.xlabel('Nota')
plt.ylabel('Frecuencia')

# Mostrar el gráfico
plt.show()

sns.countplot(data=df_filtrado, x='time_signature')

df_filtrado['mode'].value_counts().plot(kind='bar', color=['blue', 'orange'], rot=0)
plt.xlabel('Modo')
plt.ylabel('Número de canciones')
plt.xticks([0, 1], ['Mayor', 'Menor'])
plt.show()

df['instrumentalness'].value_counts()

"""Conclusiones:

Insights
*El lenguaje explícito hace que la popularidad, la danzabilidad y la energía aumenten.
*En la estructura musical, las primeras cuatro notas mantienen una dispersión acumulada, pero en las más populares las notas cambian en tres de cuatro. Siendo, DO# la más utilizada.
*La Valencia, o estado de ánimo no parece mantener una corelación directa con la popularidad, ya que baja de 0,55 a 0,44 en las más populares.
*El Time Signature se mantiene en general y los modos también, prevaleciendo los mayores.
*Parecería haber una correlación inversa entre la acústica y la instrumentalidad con la popularidad.
*La media de duración de las canciones populares es de 200’ y se mantiene cercana a la media general que es de 230.
Recomendaciones:
*Si lo que se busca es posicionar mejor una pista en Spotify, se sugiere trabajar en composiciones en 4 tiempos, utilizando las notas DO#, SI, DO y FA#, que contengan lenguaje explícito y una alta energía y bailabilidad.
*Se sugiere reducir el uso de instrumentales y acústicos, mantener una duración de no más de 200’ y no abusar del uso de lenguaje hablado.
*Tener en cuenta que Spotify mide la popularidad entre las reproducciones totales y las actuales, por tanto, mantener actualizado el catálogo de un artista tiene una correlación directa con su popularidad en la app

#Regresion Lineal y Regresion Lineal Multiple
"""

#importamos las librerias necesarias
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from scipy.stats import ttest_ind
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# Seleccionar las características (features) y la variable objetivo (target)
features = ['explicit', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']
target = 'popularity'

# Dividir los datos en características (X) y variable objetivo (y)
X = df_sample[features]
y = df_sample[target]

# ColumnTransformer para codificar la variable categórica "key"
column_transformer = ColumnTransformer(
    [('encoder', OneHotEncoder(), ['key'])],  # columnas a codificar
    remainder='passthrough'  # mantener las demás columnas sin modificar
)
X_encoded = column_transformer.fit_transform(X)

# Escalar las características utilizando StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)


# Dividir los datos escalados en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Entrenar el modelo de regresión lineal
regression_model = LinearRegression()
regression_model.fit(X_train, y_train)

# Realizar predicciones en el conjunto de prueba
y_pred = regression_model.predict(X_test)

# Evaluar el modelo utilizando varias métricas de regresión
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print('Error cuadrático medio (MSE):', mse)
print('Error absoluto medio (MAE):', mae)
print('Coeficiente de determinación (R^2):', r2)

# Visualizar los resultados
plt.scatter(y_test, y_pred)
plt.xlabel('Valor verdadero')
plt.ylabel('Predicción')
plt.title('Comparación entre los valores verdaderos y las predicciones')
plt.show()

"""*Ahora lo hacemos sobre el data set filtrado o las 1000 pistas populares"""

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# Seleccionar las características (features) y la variable objetivo (target)
features = ['explicit', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature']
target = 'popularity'

# Dividir los datos en características (X) y variable objetivo (y)
X = df_filtrado[features]
y = df_filtrado[target]

# ColumnTransformer para codificar la variable categórica "key"
column_transformer = ColumnTransformer(
    [('encoder', OneHotEncoder(), ['key'])],  # columnas a codificar
    remainder='passthrough'  # mantener las demás columnas sin modificar
)
X_encoded = column_transformer.fit_transform(X)

# Escalar las características utilizando StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)


# Dividir los datos escalados en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Entrenar el modelo de regresión lineal
regression_model = LinearRegression()
regression_model.fit(X_train, y_train)

# Realizar predicciones en el conjunto de prueba
y_pred = regression_model.predict(X_test)

# Evaluar el modelo utilizando varias métricas de regresión
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print('Error cuadrático medio (MSE):', mse)
print('Error absoluto medio (MAE):', mae)
print('Coeficiente de determinación (R^2):', r2)

# Visualizar los resultados
plt.scatter(y_test, y_pred)
plt.xlabel('Valor verdadero')
plt.ylabel('Predicción')
plt.title('Comparación entre los valores verdaderos y las predicciones')
plt.show()

"""En términos de los errores cuadráticos medios y los errores absolutos medios, el modelo ajustado en df_filtrado muestra un mejor rendimiento en comparación con el modelo en df. Sin embargo, es importante tener en cuenta que los valores de R^2 en ambos casos son bajos, lo que sugiere que ambos modelos tienen dificultades para explicar la variabilidad en la popularidad. Puede ser necesario realizar ajustes adicionales en los modelos o explorar enfoques alternativos para mejorar su capacidad de predicción de la popularidad"""

#continua...

